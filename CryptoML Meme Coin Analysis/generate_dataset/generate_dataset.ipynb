{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "directory_meme = 'whitepapers/meme'\n",
    "directory_other = 'whitepapers/otherCrypto'\n",
    "\n",
    "obj = os.scandir()\n",
    "\n",
    "dataset = []\n",
    "count_meme = 0\n",
    "count_other = 0\n",
    "\n",
    "# scan meme directory\n",
    "for entry in os.scandir(directory_meme):\n",
    "    with open(entry.path) as f:\n",
    "        data = f.read().splitlines()\n",
    "        temp = ' '.join(data)\n",
    "        dataset.append(temp)\n",
    "        count_meme += 1 \n",
    "\n",
    "# scan otherCrypto directory\n",
    "for entry in os.scandir(directory_other):\n",
    "    with open(entry.path) as f:\n",
    "        data = f.read().splitlines()\n",
    "        temp = ' '.join(data)\n",
    "        dataset.append(temp)\n",
    "        count_other += 1 \n",
    "\n",
    "# extract only words from whitepaper text\n",
    "def word_extraction(sentence):\n",
    "    ignore = ['a', \"the\", \"is\"]\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
    "    return cleaned_text\n",
    "\n",
    "# tokenize cleaned whitepaper text\n",
    "def tokenize(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = word_extraction(sentence)\n",
    "        words.extend(w)\n",
    "        words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "# generate bag of words\n",
    "def generate_bow(allsentences, count_total, count_label1, count_label2):\n",
    "\n",
    "    vocab = tokenize(allsentences)\n",
    "    vocab = vocab[1347:] # remove numbers from vocab\n",
    "    vectorized_ds = np.zeros((count_total, len(vocab)+1))\n",
    "    ind = 0\n",
    "\n",
    "    for sentence in allsentences:\n",
    "        words = word_extraction(sentence)\n",
    "        bag_vector = np.zeros((1, len(vocab)+1))\n",
    "        for w in words:\n",
    "            for i,word in enumerate(vocab):\n",
    "                if word == w:\n",
    "                    bag_vector[0, i+1] += 1\n",
    "        if (ind < count_total + count_label1):\n",
    "            label = 0 # meme\n",
    "        else:\n",
    "            label = 1 #meme\n",
    "        bag_vector[0, 0] = label\n",
    "        vectorized_ds = np.concatenate((vectorized_ds, bag_vector), axis=0)\n",
    "\n",
    "    vectorized_ds = vectorized_ds[0:count_total,:]\n",
    "    vocab = np.array(vocab)\n",
    "    return vocab, vectorized_ds\n",
    "\n",
    "count_total = count_meme + count_other\n",
    "vocab, data = generate_bow(dataset, count_total, count_meme, count_other)\n",
    "\n",
    "# print labels and data to csv\n",
    "# label_csv = np.savetxt(\"labels.csv\", data, delimiter=\",\")\n",
    "# data_csv = np.savetxt(\"bow_data.csv\", data, delimiter=\",\")\n",
    "\n",
    "obj.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21129,)\n",
      "(80, 21130)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f686ea186f7765d19b2e4eda4390df13e09291e6f40162ffbe7cb564c97cc4f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}